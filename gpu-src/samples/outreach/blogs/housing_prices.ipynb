{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the use `tf.feature_column.crossed_column` on some simulated Atlanta housing price data. \n",
    "This spatial data is used primarily so the results can be easily visualized. \n",
    "\n",
    "These functions are designed primarily for categorical data, not to build interpolation tables. \n",
    "\n",
    "If you actually want to build smart interpolation tables in TensorFlow you may want to consider [TensorFlow Lattice](https://research.googleblog.com/2017/10/tensorflow-lattice-flexibility.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.VERSION.split('.') >= ['1','4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = 12, 6\n",
    "mpl.rcParams['image.cmap'] = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TensorBoard\n",
    "The following command will kill all running TensorBoard processes, and start a new one monitoring to the above logdir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.Popen(['pkill','-f','tensorboard'])\n",
    "subprocess.Popen(['tensorboard', '--logdir', logdir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "min_latitude = 33.641336\n",
    "max_latitude = 33.887157\n",
    "delta_latitude = max_latitude-min_latitude\n",
    "\n",
    "min_longitude = -84.558798\n",
    "max_longitude = -84.287259\n",
    "delta_longitude = max_longitude-min_longitude\n",
    "\n",
    "resolution = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RandomState so the behavior is repeatable. \n",
    "R = np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The price data will be a sum of Gaussians, at random locations.\n",
    "n_centers = 20\n",
    "centers = R.rand(n_centers, 2)  # shape: (centers, dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Gaussian has a maximum price contribution, at the center.\n",
    "# Price_\n",
    "price_delta = 0.5+2*R.rand(n_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Gaussian also has a standard-deviation and variance.\n",
    "std = 0.2*R.rand(n_centers)  # shape: (centers)\n",
    "var = std**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(latitude, longitude):\n",
    "    # Convert latitude, longitude to x,y in [0,1]\n",
    "    x = (longitude - min_longitude)/delta_longitude\n",
    "    y = (latitude - min_latitude)/delta_latitude\n",
    "    \n",
    "    # Cache the shape, and flatten the inputs.\n",
    "    shape = x.shape\n",
    "    assert y.shape == x.shape\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    \n",
    "    # Convert x, y examples into an array with shape (examples, dimensions)\n",
    "    xy = np.array([x,y]).T\n",
    "\n",
    "    # Calculate the square distance from each example to each center.  \n",
    "    components2 = (xy[:,None,:] - centers[None,:,:])**2  # shape: (examples, centers, dimensions)\n",
    "    r2 = components2.sum(axis=2)  # shape: (examples, centers)\n",
    "    \n",
    "    # Calculate the z**2 for each example from each center.\n",
    "    z2 = r2/var[None,:]\n",
    "    price = (np.exp(-z2)*price_delta).sum(1)  # shape: (examples,)\n",
    "    \n",
    "    # Restore the original shape.\n",
    "    return price.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the grid. We want `resolution` cells between `min` and `max` on each dimension\n",
    "# so we need `resolution+1` evenly spaced edges. The centers are at the average of the\n",
    "# upper and lower edge. \n",
    "\n",
    "latitude_edges = np.linspace(min_latitude, max_latitude, resolution+1)\n",
    "latitude_centers = (latitude_edges[:-1] + latitude_edges[1:])/2\n",
    "\n",
    "longitude_edges = np.linspace(min_longitude, max_longitude, resolution+1)\n",
    "longitude_centers = (longitude_edges[:-1] + longitude_edges[1:])/2\n",
    "\n",
    "latitude_grid, longitude_grid = np.meshgrid(\n",
    "    latitude_centers,\n",
    "    longitude_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the price at each center-point\n",
    "actual_price_grid = price(latitude_grid, longitude_grid)\n",
    "\n",
    "price_min = actual_price_grid.min()\n",
    "price_max = actual_price_grid.max()\n",
    "price_mean = actual_price_grid.mean()\n",
    "price_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_price(price):\n",
    "    plt.imshow(\n",
    "        price, \n",
    "        # The color axis goes from `price_min` to `price_max`.\n",
    "        vmin=price_min, vmax=price_max,\n",
    "        # Put the image at the correct latitude and longitude.\n",
    "        extent=(min_longitude, max_longitude, min_latitude, max_latitude), \n",
    "        # Make the image square.\n",
    "        aspect = 1.0*delta_longitude/delta_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_price(actual_price_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test data we will use the grid centers.\n",
    "test_features = {'latitude':latitude_grid.flatten(), 'longitude':longitude_grid.flatten()}\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_features, \n",
    "                                           actual_price_grid.flatten()))\n",
    "test_ds = test_ds.cache().batch(512).prefetch(1)\n",
    "\n",
    "# For training data we will use a set of random points.\n",
    "train_latitude = min_latitude + np.random.rand(50000)*delta_latitude\n",
    "train_longitude = min_longitude + np.random.rand(50000)*delta_longitude\n",
    "train_price = price(train_latitude, train_longitude)\n",
    "\n",
    "train_features = {'latitude':train_latitude, 'longitude':train_longitude}\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_price))\n",
    "train_ds = train_ds.cache().repeat().shuffle(100000).batch(512).prefetch(1)\n",
    "\n",
    "# A shortcut to build an `input_fn` from a `Dataset`\n",
    "def in_fn(ds):\n",
    "    return lambda : ds.make_one_shot_iterator().get_next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a plot from an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_est(est, ds = test_ds):\n",
    "    # Create two plot axes\n",
    "    actual, predicted = plt.subplot(1,2,1), plt.subplot(1,2,2)\n",
    "\n",
    "    # Plot the actual price.\n",
    "    plt.sca(actual)\n",
    "    show_price(actual_price_grid.reshape(resolution, resolution))\n",
    "    \n",
    "    # Generate predictions over the grid from the estimator.\n",
    "    pred =  est.predict(in_fn(ds))\n",
    "    # Convert them to a numpy array.\n",
    "    pred = np.fromiter((item['predictions'] for item in pred), np.float32)\n",
    "    # Plot the predictions on the secodn axis.\n",
    "    plt.sca(predicted)\n",
    "    show_price(pred.reshape(resolution, resolution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `numeric_column` with DNNRegressor\n",
    "Important: Pure categorical data doesn't the spatial relationships that make this example possible. Embeddings are a way your model can learn spatial relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `normalizer_fn` so that the model only sees values in [0, 1]\n",
    "norm_latitude = lambda latitude:(latitude-min_latitude)/delta_latitude - 0.5\n",
    "norm_longitude = lambda longitude:(longitude-min_longitude)/delta_longitude - 0.5\n",
    "\n",
    "fc = [tf.feature_column.numeric_column('latitude', normalizer_fn = norm_latitude), \n",
    "      tf.feature_column.numeric_column('longitude', normalizer_fn = norm_longitude)]\n",
    "\n",
    "# Build and train the Estimator\n",
    "est = tf.estimator.DNNRegressor(\n",
    "    hidden_units=[100,100], \n",
    "    feature_columns=fc, \n",
    "    model_dir = os.path.join(logdir,'DNN'))\n",
    "\n",
    "est.train(in_fn(train_ds), steps = 5000)\n",
    "est.evaluate(in_fn(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_est(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `bucketized_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketize the latitude and longitude usig the `edges`\n",
    "latitude_bucket_fc = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('latitude'), \n",
    "    list(latitude_edges))\n",
    "\n",
    "longitude_bucket_fc = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('longitude'),\n",
    "    list(longitude_edges))\n",
    "\n",
    "fc = [\n",
    "    latitude_bucket_fc,\n",
    "    longitude_bucket_fc]\n",
    "\n",
    "# Build and train the Estimator.\n",
    "est = tf.estimator.LinearRegressor(fc, model_dir = os.path.join(logdir,'separable'))\n",
    "est.train(in_fn(train_ds), steps = 5000)\n",
    "est.evaluate(in_fn(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_est(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `crossed_column` on its own.\n",
    "The single-cell \"holes\" in the figure are caused by cells which do not contain examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross the bucketized columns, using 5000 hash bins (for an average weight sharing of 2).\n",
    "crossed_lat_lon_fc = tf.feature_column.crossed_column(\n",
    "    [latitude_bucket_fc, longitude_bucket_fc], int(5e3))\n",
    "\n",
    "fc = [crossed_lat_lon_fc]\n",
    "\n",
    "# Build and train the Estimator.\n",
    "est = tf.estimator.LinearRegressor(fc, model_dir=os.path.join(logdir, 'crossed'))\n",
    "\n",
    "est.train(in_fn(train_ds), steps = 5000)\n",
    "est.evaluate(in_fn(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_est(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using raw categories with `crossed_column` \n",
    "The model generalizes better if it also has access to the raw categories, outside of the cross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = [\n",
    "    latitude_bucket_fc,\n",
    "    longitude_bucket_fc,\n",
    "    crossed_lat_lon_fc]\n",
    "\n",
    "# Build and train the Estimator.\n",
    "est = tf.estimator.LinearRegressor(fc, model_dir=os.path.join(logdir, 'both'))\n",
    "est.train(in_fn(train_ds), steps = 5000)\n",
    "est.evaluate(in_fn(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_est(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"900\" height=\"800\" src=\"http://0.0.0.0:6006#scalars&_smoothingWeight=0.85\" frameborder=\"0\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
